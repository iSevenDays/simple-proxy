# Claude Code Proxy Model Configuration
# 
# Configure which models and endpoints to use for Claude Code requests.
# Copy this file to .env and customize as needed.
# 
# ALL MODELS AND ENDPOINTS ARE REQUIRED - no fallbacks when .env exists

# BIG_MODEL: Used for Claude Sonnet requests (high-capability tasks)
BIG_MODEL=your-big-model-name
BIG_MODEL_ENDPOINT=http://192.168.0.24:8080/v1/chat/completions
BIG_MODEL_API_KEY=sk-your-api-key

# SMALL_MODEL: Used for Claude Haiku requests (fast, lightweight tasks)  
SMALL_MODEL=qwen2.5-coder:latest
SMALL_MODEL_ENDPOINT=http://192.168.0.46:11434/v1/chat/completions
SMALL_MODEL_API_KEY=ollama

# CORRECTION_MODEL: Used for tool call correction service
CORRECTION_MODEL=qwen2.5-coder:latest

# SKIP_TOOLS: Comma-separated list of tool names to skip/filter out (optional)
# Example: SKIP_TOOLS=NotebookRead,NotebookEdit,SomeOtherTool
SKIP_TOOLS=NotebookRead,NotebookEdit

# PRINT_SYSTEM_MESSAGE: Print system messages to logs for debugging (optional)
# Set to "true" or "1" to enable, anything else (or omit) to disable
PRINT_SYSTEM_MESSAGE=false

# Examples of other configurations:
# BIG_MODEL=kimi-k2
# BIG_MODEL_ENDPOINT=http://localhost:8080/v1/chat/completions
# BIG_MODEL_API_KEY=sk-your-kimi-key
# SMALL_MODEL=llama-3.1-8b-instruct
# SMALL_MODEL_ENDPOINT=http://localhost:11434/v1/chat/completions
# SMALL_MODEL_API_KEY=ollama
# CORRECTION_MODEL=llama-3.1-8b-instruct